Qwen-1.8B LoRAæ™ºèƒ½åŠ©æ‰‹
Python PyTorch HF
ğŸŒŸ æ ¸å¿ƒåŠŸèƒ½
åŸºäºQLoRAçš„4-bité‡åŒ–å¾®è°ƒï¼ˆèŠ‚çœ70%æ˜¾å­˜ï¼‰
æ”¯æŒåŠ¨æ€æ‰¹å¤„ç†ä¸æ¢¯åº¦æ£€æŸ¥ç‚¹
é›†æˆREST APIä¸WebSocketåŒåè®®æ¥å£
è‡ªé€‚åº”æ¸©åº¦è°ƒèŠ‚ï¼ˆ0.1-2.0ï¼‰ä¸é‡å¤æƒ©ç½šæœºåˆ¶
ğŸš€ å¿«é€Ÿå¯åŠ¨
ç¯å¢ƒè¦æ±‚
NVIDIA GPU â‰¥ RTX 3090 (24GB VRAM)
CUDA 11.8 â€¢ Ubuntu 20.04+
ä¸‰æ­¥éƒ¨ç½²
# 1. å…‹éš†ä»“åº“ 
git clone https://github.com/yourusername/qwen-lora-chat  
 
# 2. å®‰è£…ä¾èµ–ï¼ˆå»ºè®®ä½¿ç”¨è™šæ‹Ÿç¯å¢ƒï¼‰
pip install -r requirements.txt  
 
# 3. å¯åŠ¨æœåŠ¡ 
python main.py  --port 8000 --quantize
âš™ï¸ é«˜çº§é…ç½®
Dockeréƒ¨ç½²
docker build -t qwen-chat .
docker run -d --gpus all -p 8000:8000 \
  -v ./models:/app/models \
  qwen-chat --max_tokens 2048
APIè°ƒç”¨ç¤ºä¾‹
import requests 
 
payload = {
    "prompt": "ç”¨Pythonå®ç°å¿«é€Ÿæ’åº",
    "temperature": 0.9,
    "max_tokens": 500 
}
response = requests.post("http://localhost:8000/generate",  json=payload)
print(response.json()["response"])
ğŸ“ é¡¹ç›®ç»“æ„
â”œâ”€â”€ configs/            # é…ç½®æ–‡ä»¶ 
â”‚   â”œâ”€â”€ train.yaml       # è®­ç»ƒå‚æ•°é…ç½® 
â”‚   â””â”€â”€ inference.yaml   # æ¨ç†å‚æ•°é…ç½® 
â”œâ”€â”€ data/               # æ•°æ®å¤„ç†æ¨¡å— 
â”œâ”€â”€ docker/             # å®¹å™¨åŒ–é…ç½® 
â”œâ”€â”€ models/             # æ¨¡å‹æ–‡ä»¶ï¼ˆè‡ªåŠ¨ä¸‹è½½ï¼‰
â”œâ”€â”€ requirements.txt     # Pythonä¾èµ– 
â””â”€â”€ README.html          # æœ¬è¯´æ˜æ–‡ä»¶
âš ï¸ é‡è¦æç¤º
â€¢ é¦–æ¬¡è¿è¡Œä¼šè‡ªåŠ¨ä¸‹è½½çº¦8GBçš„é¢„è®­ç»ƒæ¨¡å‹æ–‡ä»¶

â€¢ å»ºè®®ä½¿ç”¨Git LFSç®¡ç†å¤§æ–‡ä»¶ï¼š

git lfs install 
git lfs track "*.safetensors"
git add .gitattributes
