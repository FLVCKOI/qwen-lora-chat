# Qwen-1.8B LoRAæ™ºèƒ½åŠ©æ‰‹ ğŸš€ 
 
![Python](https://img.shields.io/badge/Python-3.10%2B-blue) 
![PyTorch](https://img.shields.io/badge/PyTorch-2.2%2B-red) 
![HF](https://img.shields.io/badge/HuggingFace-Transformers-orange) 
 
## ğŸŒŸ æ ¸å¿ƒåŠŸèƒ½ 
- **é«˜æ•ˆå¾®è°ƒ**ï¼šé‡‡ç”¨4-bit QLoRAé‡åŒ–æŠ€æœ¯ï¼Œæ˜¾å­˜å ç”¨é™ä½70%
- **æ™ºèƒ½ç”Ÿæˆ**ï¼šæ”¯æŒåŠ¨æ€æ¸©åº¦è°ƒèŠ‚ï¼ˆ0.1-2.0ï¼‰ä¸2048 tokensé•¿æ–‡æœ¬ç”Ÿæˆ 
- **å¤šåè®®æ¥å£**ï¼šé›†æˆREST API + WebSocketåŒé€šä¿¡åè®® 
- **ç”Ÿäº§å°±ç»ª**ï¼šæä¾›Dockerå®¹å™¨åŒ–éƒ¨ç½²æ–¹æ¡ˆ 
 
## ğŸš€ å¿«é€Ÿéƒ¨ç½² 
### åŸºç¡€ç¯å¢ƒè¦æ±‚ 
```bash 
# æœ€ä½ç¡¬ä»¶é…ç½® 
GPUï¼šNVIDIA RTX 3090 (24GB VRAM)
CUDAï¼š11.8+
OSï¼šUbuntu 20.04 LTS
ä¸‰æ­¥å¯åŠ¨æ–¹æ¡ˆ
bash
å¤åˆ¶
git clone https://github.com/yourrepo/qwen-lora-chat  
pip install -r requirements.txt  
python main.py  --port 8000 --quantize 
âš™ï¸ é«˜çº§é…ç½®
Dockeréƒ¨ç½²
dockerfile
å¤åˆ¶
docker build -t qwen-chat .
docker run -d --gpus all -p 8000:8000 \
  -v ./models:/app/models \
  qwen-chat --max_tokens 2048 
APIè°ƒç”¨ç¤ºä¾‹
python
å¤åˆ¶
import requests 
 
payload = {
    "prompt": "ç”¨Pythonå®ç°å¿«é€Ÿæ’åº",
    "temperature": 0.9,
    "max_tokens": 500 
}
response = requests.post("http://localhost:8000/generate",  json=payload)
print(response.json()["response"]) 
ğŸ“ é¡¹ç›®ç»“æ„
.
â”œâ”€â”€ configs/            # è®­ç»ƒ/æ¨ç†é…ç½® 
â”œâ”€â”€ data/               # æ•°æ®å¤„ç†æ¨¡å— 
â”œâ”€â”€ docker/             # å®¹å™¨åŒ–é…ç½® 
â”œâ”€â”€ models/             # æ¨¡å‹å­˜å‚¨ç›®å½•ï¼ˆè‡ªåŠ¨ä¸‹è½½ï¼‰
â””â”€â”€ README.md            # æœ¬è¯´æ˜æ–‡æ¡£ 
âš ï¸ æ³¨æ„äº‹é¡¹
æ¨¡å‹ä¸‹è½½ï¼šé¦–æ¬¡è¿è¡Œä¼šè‡ªåŠ¨ä¸‹è½½çº¦8GBçš„é¢„è®­ç»ƒæ¨¡å‹
å¤§æ–‡ä»¶ç®¡ç†ï¼šå»ºè®®ä½¿ç”¨Git LFSï¼š
bash
å¤åˆ¶
git lfs install 
git lfs track "*.safetensors"
git add .gitattributes 
